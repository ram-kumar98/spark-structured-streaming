{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df31ed22-eb0a-4acc-89a3-b0cd4b6e2820",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Building the simple Word count application \n",
    "* Initially we will build the batch application with word count problem.\n",
    "* Then we will convert the batch application to stream application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47d3ec25-e9df-4a05-b5d7-bd2a237dbf72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "class WordCount():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.base_data_dir = \"/FileStore/ram\"\n",
    "        print(\"Started the process.\")\n",
    "    \n",
    "    def getRawData(self):\n",
    "        print(\"Bringing the raw data.\")\n",
    "        return ( spark.read.format('text')\n",
    "            .load(f\"{base_data_dir}/data/text/\")\n",
    "        )\n",
    "\n",
    "    def explodeData(self,df):\n",
    "        print(\"Exploding the data.\")\n",
    "        return df.select(F.explode(F.split(df.value,' ')).alias('word'))\n",
    "    \n",
    "    def qualityAnalysis(self,df):\n",
    "        print(\"Running Quality.\")\n",
    "        return ( df.select(F.lower(F.trim(df.word)).alias('word'))\n",
    "            .filter('word is not null')\n",
    "            .filter(\"word rlike '[a-z]'\")\n",
    "        )\n",
    "\n",
    "    def aggregareCount(self,df):\n",
    "        print(\"Performing Aggregation.\")\n",
    "        return (\n",
    "            df.groupBy('word').count()\n",
    "        )\n",
    "\n",
    "    def writeTable(self,df):\n",
    "        print(\"Writing the data into table.\")\n",
    "        return (\n",
    "                df.\n",
    "                write.\n",
    "                format('delta').\n",
    "                mode('overwrite')\n",
    "                .saveAsTable('word_count_table')\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def startProcess(self):\n",
    "        raw_data = self.getRawData()\n",
    "        exploded_df = self.explodeData(raw_data)\n",
    "        quality_df = self.qualityAnalysis(exploded_df)\n",
    "        count_df = self.aggregareCount(quality_df)\n",
    "        self.writeTable(count_df)\n",
    "        print(\"Done with the load\")\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a22d0f11-b24e-4617-8df7-2c6602c5164f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "WordCount().startProcess()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1.First Spark Application",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
